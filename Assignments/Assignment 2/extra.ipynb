{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5e62828a-fd26-4637-9f04-527072e706d0",
      "metadata": {},
      "source": [
        "# POP77142 Assignment 2: Text Analysis\n",
        "\n",
        "## Before Submission\n",
        "\n",
        "-   Make sure that you can run all cells without errors\n",
        "-   You can do it by clicking `Kernel`, `Restart & Run All` in the menu\n",
        "    above\n",
        "-   Make sure that you save the output by pressing Command+S / CTRL+S\n",
        "-   Rename the file from `02_assignment.ipynb` to\n",
        "    `02_lastname_firstname_studentnumber.ipynb`\n",
        "-   Use Firefox browser for submitting your Jupyter notebook on\n",
        "    Blackboard.\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this assignment you will need to analyse the debates of the 33rd\n",
        "session of the Dáil Éireann (Irish Parliament) that was in sitting\n",
        "between 2020 and 2024. The complete debate records for that session are\n",
        "available on Blackboard as a compressed CSV file. Do note that the\n",
        "dataset is quite large , it contains ~600K individual speeches and takes\n",
        "about 0.5GB of disk space when uncompressed.\n",
        "\n",
        "The dataset is structured as follows:\n",
        "\n",
        "| dail | vol | no  | date | speaker_name | speaker_role | constituency | party | text |\n",
        "|------|-----|-----|------|--------------|--------------|--------------|-------|------|\n",
        "\n",
        "where:\n",
        "\n",
        "`dail` - is the number of the Dáil (e.g. 33rd Dáil)\n",
        "\n",
        "`vol` - is the volume number of the debates (e.g. 1000)\n",
        "\n",
        "`no` - is the number of the debate in the volume (e.g. 1)\n",
        "\n",
        "`date` - is the date of the debate (in YYYY-MM-DD form, e.g. 2020-01-01)\n",
        "\n",
        "`speaker_name` - is the name of the speaker\n",
        "\n",
        "`speaker_role` - is the role of the speaker (e.g. TD, Minister, etc.)\n",
        "\n",
        "`constituency` - is the constituency of the speaker\n",
        "\n",
        "`party` - is the party of the speaker\n",
        "\n",
        "`text` - is the text of the speech\n",
        "\n",
        "Note that some of the texts belong to the outside speakers, such as,\n",
        "e.g. external experts, witnesses, etc. Another aspect of this data to\n",
        "keep in mind is that some of the recorded speeches are in Irish. You can\n",
        "choose to use those in your analysis or exclude them.\n",
        "\n",
        "## Part 1: Modelling Topics\n",
        "\n",
        "In this part of the assignment you will need to model the topics of the\n",
        "speeches in the Dáil. You can use any method that you think is most\n",
        "appropriate for this task. You can adopt any of the other number of\n",
        "avenues: dictionary methods, topic modelling, supervised learning, LLMs.\n",
        "You can also choose to use the metadata in the dataset to inform your\n",
        "analysis.\n",
        "\n",
        "## Part 2: Modelling Ideology\n",
        "\n",
        "In this part of the assignment you will need to model the ideology of\n",
        "the speakers in the Dáil. There could be a number of ways to tackle this\n",
        "problem, from more traditional methods, such as, e.g. dictionary-based\n",
        "approaches, to more advanced methods, such as, e.g. supervised learning\n",
        "and LLMs. You are free to choose the method that you think is most\n",
        "appropriate for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "54dd8f0e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\athen\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\athen\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import pyLDAvis\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b201099",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('dail_33_small.csv') \n",
        "\n",
        "# Process and Clean Text\n",
        "df = df.dropna(subset=['text'])\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\d+', '', text.lower())  # lower and remove numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)      # remove punctuation\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['clean_text'] = df['text'].astype(str).apply(clean_text)\n",
        "df.head()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "new_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
